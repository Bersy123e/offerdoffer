import pandas as pd
import re
import logging
import numpy as np
from typing import List, Dict, Optional, Any, Tuple
import os
import json
import csv
import docx
import tabula # –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü –∏–∑ PDF
import openpyxl # –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∏–ª–µ–π
from pydantic import BaseModel, ValidationError, Field

logger = logging.getLogger('commercial_proposal')

# --- Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ LLM ---

class ColumnMap(BaseModel):
    price_col_index: Optional[int] = None
    stock_col_index: Optional[int] = None
    name_parts_col_indices: List[int] = Field(..., min_items=1)

class PriceListMap(BaseModel):
    header_row_index: int
    data_start_row_index: int
    column_map: ColumnMap

class AuditorVerdict(BaseModel):
    is_correct: bool
    reasoning: str

class ExtractedProduct(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞, –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ LLM."""
    full_name: str = Field(..., min_length=3)
    price: Optional[float] = None
    stock: str = "–≤ –Ω–∞–ª–∏—á–∏–∏"

class CascadeProcessor:
    """
    –£–º–Ω—ã–π –∫–∞—Å–∫–∞–¥–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä "–•–∏—Ä—É—Ä–≥ 3.0" –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–æ–≤.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LLM –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.
    """

    def __init__(self, llm: Any):
        if not llm:
            raise ValueError("LLM instance is required.")
        self.llm = llm
        self.cascade_log = []

    def process_file_cascade(self, file_path: str, file_name: str) -> Dict:
        """
        –ù–∞–¥–µ–∂–Ω–∞—è –∫–∞—Å–∫–∞–¥–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å —Ç—Ä–µ–º—è —É—Ä–æ–≤–Ω—è–º–∏ –∏ –ø–æ–ª–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
        
        –£–†–û–í–ï–ù–¨ 1: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LLM (–¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä)
        –£–†–û–í–ï–ù–¨ 2: –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LLM + –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ (–¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü)
        –£–†–û–í–ï–ù–¨ 3: –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã (fallback –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Å–ª—É—á–∞–µ–≤)
        """
        logger.info(f"--- –ó–ê–ü–£–°–ö –ö–ê–°–ö–ê–î–ù–û–ô –°–ò–°–¢–ï–ú–´ –¥–ª—è —Ñ–∞–π–ª–∞: {file_name} ---")
        self.cascade_log = [f"–ù–∞—á–∞–ª–æ –∫–∞—Å–∫–∞–¥–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {file_name}"]
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
            ext = os.path.splitext(file_path)[1].lower()
            self.cascade_log.append(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {ext}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Ä–æ–≤–Ω–µ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫ UndefinedLocalVariable
            level1_result = {"success": False, "products": [], "error": "–£—Ä–æ–≤–µ–Ω—å 1 –Ω–µ –±—ã–ª –∑–∞–ø—É—â–µ–Ω –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –∏–ª–∏ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."}
            level2_result = {"success": False, "products": [], "error": "–£—Ä–æ–≤–µ–Ω—å 2 –Ω–µ –±—ã–ª –∑–∞–ø—É—â–µ–Ω –∏–ª–∏ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."}
            level3_result = {"success": False, "products": [], "error": "–£—Ä–æ–≤–µ–Ω—å 3 –Ω–µ –±—ã–ª –∑–∞–ø—É—â–µ–Ω –∏–ª–∏ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."}

            # === –£–†–û–í–ï–ù–¨ 1: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LLM ===
            if ext in ['.xlsx', '.xls']:
                self.cascade_log.append("üî• –£–†–û–í–ï–ù–¨ 1: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LLM")
                level1_result = self._process_level1_spatial(file_path, file_name)
                
                if level1_result["success"] and len(level1_result["products"]) >= 5:
                    self.cascade_log.append(f"‚úÖ –£–†–û–í–ï–ù–¨ 1 –£–°–ü–ï–®–ï–ù: {len(level1_result['products'])} —Ç–æ–≤–∞—Ä–æ–≤")
                    level1_result["final_method"] = "Level 1: LLM Spatial Analysis"
                    level1_result["cascade_log"] = self.cascade_log
                    return level1_result
            else:
                    self.cascade_log.append(f"‚ö†Ô∏è –£–†–û–í–ï–ù–¨ 1: {len(level1_result.get('products', []))} —Ç–æ–≤–∞—Ä–æ–≤ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)")
                    # –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –£—Ä–æ–≤–Ω–µ–º 1 –∏–ª–∏ –Ω–µ –¥–∞–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞,
                    # level1_result —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –æ—à–∏–±–∫—É –±–ª–∞–≥–æ–¥–∞—Ä—è –Ω–∞—á–∞–ª—å–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
                    # –ù–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–º else-–±–ª–æ–∫–µ –∑–¥–µ—Å—å.
            
            # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞, –∑–Ω–∞—á–∏—Ç –£–†–û–í–ï–ù–¨ 1 –ª–∏–±–æ –Ω–µ –±—ã–ª –∑–∞–ø—É—â–µ–Ω (–Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç),
            # –ª–∏–±–æ –Ω–µ –¥–∞–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–≤–∞—Ä–æ–≤. level1_result —É–∂–µ –∏–º–µ–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ
            # –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.

            # === –£–†–û–í–ï–ù–¨ 2: –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LLM ===
            self.cascade_log.append("üî• –£–†–û–í–ï–ù–¨ 2: –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LLM")
            level2_result = self._process_level2_structural(file_path, file_name)
            
            if level2_result["success"] and len(level2_result["products"]) >= 3:
                self.cascade_log.append(f"‚úÖ –£–†–û–í–ï–ù–¨ 2 –£–°–ü–ï–®–ï–ù: {len(level2_result['products'])} —Ç–æ–≤–∞—Ä–æ–≤")
                level2_result["final_method"] = "Level 2: LLM Structural Analysis"
                level2_result["cascade_log"] = self.cascade_log
                return level2_result
            else:
                self.cascade_log.append(f"‚ö†Ô∏è –£–†–û–í–ï–ù–¨ 2: {len(level2_result.get('products', []))} —Ç–æ–≤–∞—Ä–æ–≤")
            
            # === –£–†–û–í–ï–ù–¨ 3: –≠–í–†–ò–°–¢–ò–ß–ï–°–ö–ò–ï –ú–ï–¢–û–î–´ ===
            self.cascade_log.append("üî• –£–†–û–í–ï–ù–¨ 3: –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã (fallback)")
            level3_result = self._process_level3_heuristics(file_path, file_name)
            
            if level3_result["success"] and len(level3_result["products"]) > 0:
                self.cascade_log.append(f"‚úÖ –£–†–û–í–ï–ù–¨ 3 –£–°–ü–ï–®–ï–ù: {len(level3_result['products'])} —Ç–æ–≤–∞—Ä–æ–≤")
                level3_result["final_method"] = "Level 3: Heuristic Methods"
                level3_result["cascade_log"] = self.cascade_log
                return level3_result
            else:
                self.cascade_log.append(f"‚ùå –£–†–û–í–ï–ù–¨ 3: {len(level3_result.get('products', []))} —Ç–æ–≤–∞—Ä–æ–≤")
            
            # === –í–´–ë–û–† –õ–£–ß–®–ï–ì–û –†–ï–ó–£–õ–¨–¢–ê–¢–ê ===
            results = []
            if level1_result.get("success"): results.append(("Level 1", level1_result))
            if level2_result.get("success"): results.append(("Level 2", level2_result))
            if level3_result.get("success"): results.append(("Level 3", level3_result))
            
            if results:
                # –í—ã–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–æ–≤–∞—Ä–æ–≤
                best_name, best_result = max(results, key=lambda x: len(x[1].get("products", [])))
                self.cascade_log.append(f"‚úÖ –í–´–ë–†–ê–ù –õ–£–ß–®–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢: {best_name} —Å {len(best_result['products'])} —Ç–æ–≤–∞—Ä–∞–º–∏")
                best_result["final_method"] = f"Best of All: {best_name}"
                best_result["cascade_log"] = self.cascade_log
                return best_result
            
            # –ï—Å–ª–∏ –≤—Å–µ –º–µ—Ç–æ–¥—ã –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å
            error_msg = "–í—Å–µ —É—Ä–æ–≤–Ω–∏ –∫–∞—Å–∫–∞–¥–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞."
            return self._handle_error(error_msg)
            
        except Exception as e:
            return self._handle_error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –∫–∞—Å–∫–∞–¥–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ: {e}", exc_info=True)

    def _process_level1_spatial(self, file_path: str, file_name: str) -> Dict:
        """–£–†–û–í–ï–ù–¨ 1: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LLM –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä."""
        try:
            # –®–∞–≥ 1: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –≤ "–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π" JSON
            spatial_json = self._file_to_spatial_json(file_path)
            if not spatial_json:
                return {"success": False, "products": [], "error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ"}

            # –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ –æ—Ç LLM
            products_json = self._get_products_from_llm(spatial_json)
            if not products_json:
                return {"success": False, "products": [], "error": "LLM –Ω–µ —Å–º–æ–≥–ª–∞ –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ JSON"}

            # –®–∞–≥ 3: –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            validated_products = self._validate_and_clean_products(products_json, "Level 1")
            
            if not validated_products:
                return {"success": False, "products": [], "error": "–ù–∏ –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é"}

            return {"success": True, "products": validated_products}
            
        except Exception as e:
            return {"success": False, "products": [], "error": f"–û—à–∏–±–∫–∞ —É—Ä–æ–≤–Ω—è 1: {e}"}

    def _process_level2_structural(self, file_path: str, file_name: str) -> Dict:
        """–£–†–û–í–ï–ù–¨ 2: –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LLM –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü."""
        try:
            # –®–∞–≥ 1: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –≤ DataFrame
            df = self._file_to_dataframe(file_path)
            if df is None:
                return {"success": False, "products": [], "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª"}

            # –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ç LLM
            structure_map, sample_text = self._get_structure_map_from_llm(df)
            if not structure_map:
                return {"success": False, "products": [], "error": "LLM –Ω–µ —Å–º–æ–≥–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–∞"}

            # –®–∞–≥ 3: –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—Ä—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            try:
                validated_map = PriceListMap(**structure_map)
            except ValidationError as e:
                return {"success": False, "products": [], "error": f"–ö–∞—Ä—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é: {e}"}

            # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ä—Ç—ã –∞—É–¥–∏—Ç–æ—Ä–æ–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if self.llm:
                verdict = self._get_auditor_verdict(sample_text, structure_map)
                if verdict and not verdict.is_correct:
                    self.cascade_log.append(f"–ê—É–¥–∏—Ç–æ—Ä –æ—Ç–∫–ª–æ–Ω–∏–ª –∫–∞—Ä—Ç—É: {verdict.reasoning}")
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º, –Ω–æ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º

            # –®–∞–≥ 5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –∫–∞—Ä—Ç–µ
            raw_products = self._extract_products_with_map(df, validated_map)
            if not raw_products:
                return {"success": False, "products": [], "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–æ–≤–∞—Ä—ã –ø–æ –∫–∞—Ä—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"}

            # –®–∞–≥ 6: –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            validated_products = self._validate_and_clean_products(raw_products, "Level 2")
            
            if not validated_products:
                return {"success": False, "products": [], "error": "–ù–∏ –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é"}

            return {"success": True, "products": validated_products}
            
        except Exception as e:
            return {"success": False, "products": [], "error": f"–û—à–∏–±–∫–∞ —É—Ä–æ–≤–Ω—è 2: {e}"}

    def _process_level3_heuristics(self, file_path: str, file_name: str) -> Dict:
        """–£–†–û–í–ï–ù–¨ 3: –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã (fallback)."""
        try:
            result = self._process_with_heuristics(file_path)
            if result["success"]:
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                validated_products = self._validate_and_clean_products(result["products"], "Level 3")
                return {"success": True, "products": validated_products}
            else:
                return result
                
        except Exception as e:
            return {"success": False, "products": [], "error": f"–û—à–∏–±–∫–∞ —É—Ä–æ–≤–Ω—è 3: {e}"}

    def _validate_and_clean_products(self, products: List[Dict], level_name: str) -> List[Dict]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π.
        
        –í—ã–ø–æ–ª–Ω—è–µ—Ç:
        1. Pydantic –≤–∞–ª–∏–¥–∞—Ü–∏—é
        2. –ü—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        3. –û—á–∏—Å—Ç–∫—É –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
        4. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        validated_products = []
        skipped_count = 0
        
        for i, item in enumerate(products):
            try:
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
                normalized_item = self._normalize_product_structure(item)
                
                # Pydantic –≤–∞–ª–∏–¥–∞—Ü–∏—è
                validated_product = ExtractedProduct(**normalized_item)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
                if self._is_quality_product(validated_product):
                    validated_products.append(validated_product.dict())
                else:
                    skipped_count += 1
                    self.cascade_log.append(f"{level_name}: –ü—Ä–æ–ø—É—â–µ–Ω —Ç–æ–≤–∞—Ä –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞: {normalized_item.get('full_name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]}")
                    
            except ValidationError as e:
                skipped_count += 1
                self.cascade_log.append(f"{level_name}: –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–æ–≤–∞—Ä–∞ {i+1}: {e}")
                continue
            except Exception as e:
                skipped_count += 1
                self.cascade_log.append(f"{level_name}: –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–æ–≤–∞—Ä–∞ {i+1}: {e}")
                continue
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        unique_products = self._remove_duplicates(validated_products)
        
        removed_duplicates = len(validated_products) - len(unique_products)
        if removed_duplicates > 0:
            self.cascade_log.append(f"{level_name}: –£–¥–∞–ª–µ–Ω–æ {removed_duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        self.cascade_log.append(f"{level_name}: –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü—Ä–∏–Ω—è—Ç–æ: {len(unique_products)}, –û—Ç–∫–ª–æ–Ω–µ–Ω–æ: {skipped_count}")
        
        return unique_products

    def _normalize_product_structure(self, item: Dict) -> Dict:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏."""
        normalized = {
            "full_name": "",
            "price": None,
            "stock": "–≤ –Ω–∞–ª–∏—á–∏–∏"
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
        if "full_name" in item:
            normalized["full_name"] = str(item["full_name"]).strip()
        elif "name" in item:
            normalized["full_name"] = str(item["name"]).strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
        if "price" in item and item["price"] is not None:
            try:
                if isinstance(item["price"], (int, float)):
                    normalized["price"] = float(item["price"])
                else:
                    # –ü—ã—Ç–∞–µ–º—Å—è –æ—á–∏—Å—Ç–∏—Ç—å –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫—É
                    cleaned_price = self._clean_price(item["price"])
                    normalized["price"] = cleaned_price
            except:
                normalized["price"] = None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å—Ç–∞—Ç–æ–∫
        if "stock" in item and item["stock"] is not None:
            normalized["stock"] = self._clean_stock(item["stock"])
        
        return normalized

    def _is_quality_product(self, product: ExtractedProduct) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–∞ –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º.
        
        –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞:
        1. –ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –º–µ–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤
        2. –ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª—É–∂–µ–±–Ω—ã–º —Å–ª–æ–≤–æ–º
        3. –¶–µ–Ω–∞ —Ä–∞–∑—É–º–Ω–∞—è (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
        4. –ù–∞–∑–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–Ω–∞—á–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        """
        name = product.full_name.lower().strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        if len(name) < 3:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
        service_words = [
            'nan', 'none', 'null', 'undefined', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '—Ç–æ–≤–∞—Ä', '–ø—Ä–æ–¥—É–∫—Ç',
            '–Ω–∞–∑–≤–∞–Ω–∏–µ', '–æ–ø–∏—Å–∞–Ω–∏–µ', '–∏—Ç–æ–≥–æ', '–≤—Å–µ–≥–æ', '—Å—É–º–º–∞', 'total', 'sum',
            '–∑–∞–≥–æ–ª–æ–≤–æ–∫', 'header', 'title', '–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ', 'note', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π'
        ]
        
        if name in service_words:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        if len(name.split()) < 2 and not any(char.isdigit() for char in name):
            # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: –µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã, –æ–Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ö—Ä–∞–Ω DN50")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω—ã (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
        if product.price is not None:
            if product.price < 0 or product.price > 10000000:  # –†–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        if len(name.replace(' ', '')) < 3:
            return False
        
        return True

    def _remove_duplicates(self, products: List[Dict]) -> List[Dict]:
        """–£–¥–∞–ª—è–µ—Ç –ò–°–¢–ò–ù–ù–´–ï –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–æ–≤–∞—Ä–æ–≤ (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏–µ + —Ü–µ–Ω–∞ + –æ—Å—Ç–∞—Ç–æ–∫)."""
        seen_combinations = set()
        unique_products = []
        
        for product in products:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è, —Ü–µ–Ω—ã –∏ –æ—Å—Ç–∞—Ç–∫–∞
            name_key = product["full_name"].lower().strip()
            price_key = str(product.get("price", ""))
            stock_key = str(product.get("stock", ""))
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª—é—á –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            combination_key = f"{name_key}|{price_key}|{stock_key}"
            
            if combination_key not in seen_combinations:
                seen_combinations.add(combination_key)
                unique_products.append(product)
        
        return unique_products

    def _handle_error(self, message: str, exc_info=False) -> Dict:
        """–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫."""
        logger.error(message, exc_info=exc_info)
        self.cascade_log.append(f"–û–®–ò–ë–ö–ê: {message}")
        return {"success": False, "products": [], "error": message, "cascade_log": self.cascade_log}

    def _file_to_spatial_json(self, file_path: str) -> Optional[str]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ñ–∞–π–ª –ª—é–±–æ–≥–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –≤ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON 
        —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ –∏ —Å—Ç–∏–ª—è–º–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
        """
        self.cascade_log.append("–®–∞–≥ 1: –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ JSON –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.")
        ext = os.path.splitext(file_path)[1].lower()
        
        try:
            if ext in ['.xlsx', '.xls']:
                # Excel —Ñ–∞–π–ª—ã - –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ –∏ —Å—Ç–∏–ª—è–º–∏
                return self._excel_to_spatial_json(file_path)
            
            elif ext == '.csv':
                # CSV —Ñ–∞–π–ª—ã - —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                return self._csv_to_spatial_json(file_path)
            
            elif ext == '.pdf':
                # PDF —Ñ–∞–π–ª—ã - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π JSON
                return self._pdf_to_spatial_json(file_path)
            
            elif ext == '.docx':
                # DOCX —Ñ–∞–π–ª—ã - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π JSON
                return self._docx_to_spatial_json(file_path)
            
            else:
                self.cascade_log.append(f"–§–æ—Ä–º–∞—Ç {ext} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.")
                return None
                
        except Exception as e:
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ JSON: {e}")
            return None

    def _excel_to_spatial_json(self, file_path: str) -> Optional[str]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç Excel —Ñ–∞–π–ª –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π JSON."""
        try:
            workbook = openpyxl.load_workbook(file_path, data_only=True)
            sheet = workbook.active
            
            cells_data = []
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤
            max_rows_to_process = 500  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ç–∏–ø–∞ –£—Ä–∞–ª–û—Ç–≤–æ–¥
            for row_idx, row in enumerate(sheet.iter_rows(max_row=max_rows_to_process)):
                for col_idx, cell in enumerate(row):
                    if cell.value is not None:
                        cells_data.append({
                            "row": row_idx, 
                            "col": col_idx,
                            "value": str(cell.value),
                            "is_bold": cell.font.b or False,
                            "is_merged": cell.coordinate in [merged_range.coord for merged_range in sheet.merged_cells.ranges]
                        })
            
            self.cascade_log.append(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(cells_data)} —è—á–µ–µ–∫ –∏–∑ Excel —Ñ–∞–π–ª–∞.")
            return json.dumps(cells_data, ensure_ascii=False)
            
        except Exception as e:
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ Excel —Ñ–∞–π–ª–∞: {e}")
            return None

    def _csv_to_spatial_json(self, file_path: str) -> Optional[str]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç CSV —Ñ–∞–π–ª –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π JSON."""
        try:
            # –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                sample = f.read(1024)
                sniffer = csv.Sniffer()
                try:
                    dialect = sniffer.sniff(sample, delimiters=',;')
                    sep = dialect.delimiter
                except csv.Error:
                    sep = ','
            
            # –ß–∏—Ç–∞–µ–º CSV
            df = pd.read_csv(file_path, header=None, sep=sep, dtype=str)
            
            cells_data = []
            for row_idx, row in df.iterrows():
                for col_idx, value in enumerate(row):
                    if pd.notna(value) and str(value).strip():
                        cells_data.append({
                            "row": row_idx,
                            "col": col_idx,
                            "value": str(value).strip(),
                            "is_bold": False,  # CSV –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                            "is_merged": False
                        })
            
            self.cascade_log.append(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(cells_data)} —è—á–µ–µ–∫ –∏–∑ CSV —Ñ–∞–π–ª–∞.")
            return json.dumps(cells_data, ensure_ascii=False)
            
        except Exception as e:
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ CSV —Ñ–∞–π–ª–∞: {e}")
            return None

    def _pdf_to_spatial_json(self, file_path: str) -> Optional[str]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç PDF —Ñ–∞–π–ª –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π JSON."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –∏–∑ PDF
            tables = tabula.read_pdf(file_path, pages='all', multiple_tables=True, pandas_options={'header': None})
            
            if not tables:
                self.cascade_log.append("–¢–∞–±–ª–∏—Ü—ã –≤ PDF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
                return None
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã
            combined_df = pd.concat(tables, ignore_index=True)
            
            cells_data = []
            for row_idx, row in combined_df.iterrows():
                for col_idx, value in enumerate(row):
                    if pd.notna(value) and str(value).strip():
                        cells_data.append({
                            "row": row_idx,
                            "col": col_idx,
                            "value": str(value).strip(),
                            "is_bold": False,  # PDF –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                            "is_merged": False
                        })
            
            self.cascade_log.append(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(cells_data)} —è—á–µ–µ–∫ –∏–∑ PDF —Ñ–∞–π–ª–∞ ({len(tables)} —Ç–∞–±–ª–∏—Ü).")
            return json.dumps(cells_data, ensure_ascii=False)
            
        except Exception as e:
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ PDF —Ñ–∞–π–ª–∞: {e}")
            return None

    def _docx_to_spatial_json(self, file_path: str) -> Optional[str]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç DOCX —Ñ–∞–π–ª –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π JSON."""
        try:
            doc = docx.Document(file_path)
            
            if not doc.tables:
                self.cascade_log.append("–¢–∞–±–ª–∏—Ü—ã –≤ DOCX –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
                return None
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É
            tables_data = []
            for table in doc.tables:
                table_data = []
                for row in table.rows:
                    table_data.append([cell.text for cell in row.cells])
                tables_data.append((len(table_data) * len(table_data[0]) if table_data else 0, table_data))
            
            if not tables_data:
                return None
            
            # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É
            _, largest_table_data = max(tables_data, key=lambda item: item[0])
            
            cells_data = []
            for row_idx, row in enumerate(largest_table_data):
                for col_idx, value in enumerate(row):
                    if value and str(value).strip():
                        cells_data.append({
                            "row": row_idx,
                            "col": col_idx,
                            "value": str(value).strip(),
                            "is_bold": False,  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–µ–∑ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                            "is_merged": False
                        })
            
            self.cascade_log.append(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(cells_data)} —è—á–µ–µ–∫ –∏–∑ DOCX —Ñ–∞–π–ª–∞.")
            return json.dumps(cells_data, ensure_ascii=False)
            
        except Exception as e:
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ DOCX —Ñ–∞–π–ª–∞: {e}")
            return None

    def _get_products_from_llm(self, spatial_json: str) -> Optional[List[Dict]]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π JSON –≤ LLM –∏ –ø–æ–ª—É—á–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤."""
        self.cascade_log.append("–®–∞–≥ 2: –ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ —É LLM.")
        prompt = self._get_spatial_analyst_prompt(spatial_json)
        
        try:
            response = self.llm.invoke(prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            self.cascade_log.append("–û—Ç–≤–µ—Ç –æ—Ç LLM –ø–æ–ª—É—á–µ–Ω.")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç –≤ markdown
            match = re.search(r'\[.*\]', response_text, re.DOTALL)
            if match:
                json_str = match.group(0)
                return json.loads(json_str)
            self.cascade_log.append("JSON-–º–∞—Å—Å–∏–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ LLM.")
            return None
        except Exception as e:
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM: {e}")
            return None

    def _get_spatial_analyst_prompt(self, spatial_json: str) -> str:
        return f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–æ–≤. –ü–µ—Ä–µ–¥ —Ç–æ–±–æ–π JSON-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –¥–∞–Ω–Ω—ã–º–∏, –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–π —è—á–µ–π–∫–∏ —É–∫–∞–∑–∞–Ω—ã –µ–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (`row`, `col`), –∑–Ω–∞—á–µ–Ω–∏–µ (`value`) –∏ —Å—Ç–∏–ª—å (`is_bold`, `is_merged`).

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –≤–µ—Ä–Ω—É—Ç—å **–≥–æ—Ç–æ–≤—ã–π, —Å–æ–±—Ä–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤.**

**–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò:**

1. **–ù–∞–π–¥–∏ –∏–µ—Ä–∞—Ä—Ö–∏—é –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É:**
   - –û–ø—Ä–µ–¥–µ–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≥—Ä—É–ø–ø (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ó–∞–¥–≤–∏–∂–∫–∏ —á—É–≥—É–Ω–Ω—ã–µ", "–§–ª–∞–Ω—Ü—ã —Å—Ç–∞–ª—å–Ω—ã–µ") - –æ–Ω–∏ —á–∞—Å—Ç–æ –≤—ã–¥–µ–ª–µ–Ω—ã –∂–∏—Ä–Ω—ã–º (`is_bold: true`) –∏–ª–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö
   - –ù–∞–π–¥–∏ —Å—Ç—Ä–æ–∫—É —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫ (–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ, –¶–µ–Ω–∞, –û—Å—Ç–∞—Ç–æ–∫, –î—É, –†—É –∏ —Ç.–¥.)
   - –û–ø—Ä–µ–¥–µ–ª–∏ –≥–¥–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–æ–≤

2. **–°–æ–±–µ—Ä–∏ –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤:**
   - –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞ —Å–æ–±–µ—Ä–∏ –ü–û–õ–ù–û–ï, –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
   - –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑: `–ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä—É–ø–ø—ã` + `–ë–∞–∑–æ–≤–æ–µ –∏–º—è` + `–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏` (–î—É, –†—É, –ì–û–°–¢, –º–∞—Ç–µ—Ä–∏–∞–ª –∏ —Ç.–¥.)
   - –ü—Ä–∏–º–µ—Ä: "–ó–∞–¥–≤–∏–∂–∫–∏ —á—É–≥—É–Ω–Ω—ã–µ" + "30—á6–±—Ä" + "–î—É 50" + "–†—É 10" = "–ó–∞–¥–≤–∏–∂–∫–∏ —á—É–≥—É–Ω–Ω—ã–µ 30—á6–±—Ä –î—É 50 –†—É 10"

3. **–ò–∑–≤–ª–µ–∫–∏ —Ü–µ–Ω—ã –∏ –æ—Å—Ç–∞—Ç–∫–∏:**
   - –ù–∞–π–¥–∏ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–Ω–∞–º–∏ (—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –æ–±—ã—á–Ω–æ 3-7 —Ü–∏—Ñ—Ä)
   - –ù–∞–π–¥–∏ –∫–æ–ª–æ–Ω–∫–∏ —Å –æ—Å—Ç–∞—Ç–∫–∞–º–∏ (–º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —á–∏—Å–ª–∞ –∏–ª–∏ —Ç–µ–∫—Å—Ç —Ç–∏–ø–∞ "–≤ –Ω–∞–ª–∏—á–∏–∏", "–ø–æ–¥ –∑–∞–∫–∞–∑")
   - –ï—Å–ª–∏ —Ü–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–π `null`
   - –ï—Å–ª–∏ –æ—Å—Ç–∞—Ç–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π "–≤ –Ω–∞–ª–∏—á–∏–∏"

4. **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è:**
   - –ù–ï –≤–∫–ª—é—á–∞–π —Å—Ç—Ä–æ–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏, –∏—Ç–æ–≥–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
   - –ù–ï –≤–∫–ª—é—á–∞–π —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü, –ø—Ä–∏–º–µ—á–∞–Ω–∏—è –∏ —Ç.–¥.)
   - –í–∫–ª—é—á–∞–π —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã —Å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏

5. **–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–Ω:**
   - –¶–µ–Ω—ã –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã, –∑–∞–ø—è—Ç—ã–µ –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á
   - –ü—Ä–µ–æ–±—Ä–∞–∑—É–π –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä: "1 234,56" ‚Üí 1234.56)
   - –ï—Å–ª–∏ —Ü–µ–Ω—É –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–π `null`

**–í–æ—Ç JSON-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞:**
---
{spatial_json}
---

**–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê - –¢–û–õ–¨–ö–û JSON-–º–∞—Å—Å–∏–≤ –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:**
```json
[
  {{
    "full_name": "–ó–∞–¥–≤–∏–∂–∫–∞ —á—É–≥—É–Ω–Ω–∞—è 30—á6–±—Ä –î—É 50 –†—É 10",
    "price": 3600.00,
    "stock": "–≤ –Ω–∞–ª–∏—á–∏–∏"
  }},
  {{
    "full_name": "–§–ª–∞–Ω–µ—Ü —Å—Ç–∞–ª—å–Ω–æ–π –ø–ª–æ—Å–∫–∏–π –î—É 100 –†—É 16 –ì–û–°–¢ 12820",
    "price": 1250.50,
    "stock": "–ø–æ–¥ –∑–∞–∫–∞–∑"
  }}
]
```

–í–ê–ñ–ù–û: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —è—á–µ–µ–∫ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã. –¢–æ–≤–∞—Ä—ã –æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ, –Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–ª–æ–Ω–æ–∫."""

    def _file_to_dataframe(self, file_path: str) -> Optional[pd.DataFrame]:
        """
        –®–∞–≥ -1: "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ü—Ä–µ-–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä".
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ñ–∞–π–ª –ª—é–±–æ–≥–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –≤ pandas DataFrame.
        """
        self.cascade_log.append("–®–∞–≥ -1: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞.")
        ext = os.path.splitext(file_path)[1].lower()
        self.cascade_log.append(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç: {ext}")
        
        try:
            if ext in ['.xlsx', '.xls', '.xlsm', '.xlsb', '.odf', '.ods', '.odt']:
                return pd.read_excel(file_path, header=None, sheet_name=0)
            
            elif ext == '.csv':
                # –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    sample = f.read(1024)
                    sniffer = csv.Sniffer()
                    try:
                        dialect = sniffer.sniff(sample, delimiters=',;')
                        sep = dialect.delimiter
                    except csv.Error:
                        sep = ',' # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                return pd.read_csv(file_path, header=None, sep=sep)

            elif ext == '.pdf':
                self.cascade_log.append("–ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü –∏–∑ PDF —Å –ø–æ–º–æ—â—å—é Tabula...")
                # Tabula –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ DataFrame'–æ–≤, –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü—ã
                tables = tabula.read_pdf(file_path, pages='all', multiple_tables=True, pandas_options={'header': None})
                if tables:
                    self.cascade_log.append(f"–ù–∞–π–¥–µ–Ω–æ {len(tables)} —Ç–∞–±–ª–∏—Ü –≤ PDF. –û–±—ä–µ–¥–∏–Ω—è–µ–º.")
                    return pd.concat(tables, ignore_index=True)
                else:
                    self.cascade_log.append("–¢–∞–±–ª–∏—Ü—ã –≤ PDF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é Tabula.")
                    return None

            elif ext == '.docx':
                self.cascade_log.append("–ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü –∏–∑ DOCX...")
                doc = docx.Document(file_path)
                if doc.tables:
                    # –ë–µ—Ä–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    tables_data = []
                    for table in doc.tables:
                        table_data = []
                        for row in table.rows:
                            table_data.append([cell.text for cell in row.cells])
                        tables_data.append((len(table_data) * len(table_data[0]) if table_data else 0, table_data))
                    
                    if tables_data:
                        # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —è—á–µ–µ–∫
                        _, largest_table_data = max(tables_data, key=lambda item: item[0])
                        return pd.DataFrame(largest_table_data)
                else:
                    self.cascade_log.append("–¢–∞–±–ª–∏—Ü—ã –≤ DOCX –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
                    return None
            
            else:
                self.cascade_log.append(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞: {ext}")
                return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞ '{file_path}' –≤ DataFrame: {e}", exc_info=True)
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            return None

    def _get_structure_map_from_llm(self, df: pd.DataFrame) -> Tuple[Optional[Dict], str]:
        """–≠—Ç–∞–ø "–†–∞–∑–≤–µ–¥—á–∏–∫". –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É DataFrame —Å –ø–æ–º–æ—â—å—é LLM-–ê–Ω–∞–ª–∏—Ç–∏–∫–∞."""
        self.cascade_log.append("–≠—Ç–∞–ø 1: –ó–∞–ø—Ä–æ—Å –∫–∞—Ä—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —É LLM-–ê–Ω–∞–ª–∏—Ç–∏–∫–∞.")
        try:
            # –®–∞–≥ 0: –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –û—Ü–µ–Ω–∫–∞
            if len(df) <= 200:
                self.cascade_log.append("–§–∞–π–ª '–º–∞–ª–µ–Ω—å–∫–∏–π' (<= 200 —Å—Ç—Ä–æ–∫). –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –Ω–∞ –∞–Ω–∞–ª–∏–∑ —Ü–µ–ª–∏–∫–æ–º.")
                sample_df = df
            else:
                self.cascade_log.append("–§–∞–π–ª '–±–æ–ª—å—à–æ–π' (> 200 —Å—Ç—Ä–æ–∫). –°–æ–∑–¥–∞–µ—Ç—Å—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞.")
                # –£–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –≤—ã–±–æ—Ä–∫–∏, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
                df_non_empty = df.dropna(how='all')
                sample_df = pd.concat([
                    df_non_empty.head(30), 
                    df_non_empty.sample(n=min(30, len(df_non_empty)), random_state=1), 
                    df_non_empty.tail(30)
                ]).drop_duplicates()

            sample_text = sample_df.to_csv(index=False, header=False, lineterminator='\n')
            prompt = self._get_llm_analyst_prompt(sample_text)
            
            self.cascade_log.append("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM-–ê–Ω–∞–ª–∏—Ç–∏–∫—É...")
            response = self.llm.invoke(prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            self.cascade_log.append("–û—Ç–≤–µ—Ç –æ—Ç LLM-–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω.")

            json_map = self._parse_llm_response(response_text)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—Ä—Ç—ã
            if not json_map or 'header_row_index' not in json_map or 'column_map' not in json_map:
                self.cascade_log.append(f"–û—à–∏–±–∫–∞: LLM –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–ø–æ–ª–Ω—É—é –∫–∞—Ä—Ç—É —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {json_map}")
                return None, ""

            self.cascade_log.append(f"–ö–∞—Ä—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–æ–±—Ä–∞–Ω–∞: {json_map}")
            return json_map, sample_text

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –∞–Ω–∞–ª–∏–∑–∞ LLM: {e}", exc_info=True)
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –∞–Ω–∞–ª–∏–∑–∞ LLM-–ê–Ω–∞–ª–∏—Ç–∏–∫–∞: {e}")
            return None, ""

    def _get_llm_analyst_prompt(self, sample_text: str) -> str:
        return f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–æ–≤ —Å–æ —Å–ª–æ–∂–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π. –ü–µ—Ä–µ–¥ —Ç–æ–±–æ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –≤–µ—Ä–Ω—É—Ç—å JSON-–æ–±—ä–µ–∫—Ç.

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**
1. –ù–æ–º–µ—Ä–∞ –∫–æ–ª–æ–Ω–æ–∫ –∏ —Å—Ç—Ä–æ–∫ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 0.
2. `header_row_index` - —ç—Ç–æ –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.
3. `data_start_row_index` - —ç—Ç–æ –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏, —Å –∫–æ—Ç–æ—Ä–æ–π –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –†–ï–ê–õ–¨–ù–´–ï —Ç–æ–≤–∞—Ä—ã.
4. `price_col_index` - —ç—Ç–æ –ò–ù–î–ï–ö–° –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–Ω–æ–π. –û–Ω–∞ –º–æ–∂–µ—Ç –Ω–∞–∑—ã–≤–∞—Ç—å—Å—è "–¶–µ–Ω–∞", "–°—Ç–æ–∏–º–æ—Å—Ç—å", "–¶–µ–Ω–∞ —Å –ù–î–°", "—Ä—É–±.". –ò—â–∏ –∫–æ–ª–æ–Ω–∫—É —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏, –ø–æ—Ö–æ–∂–∏–º–∏ –Ω–∞ –¥–µ–Ω—å–≥–∏.
5. `stock_col_index` - —ç—Ç–æ –ò–ù–î–ï–ö–° –∫–æ–ª–æ–Ω–∫–∏ —Å –æ—Å—Ç–∞—Ç–∫–æ–º. –û–Ω–∞ –º–æ–∂–µ—Ç –Ω–∞–∑—ã–≤–∞—Ç—å—Å—è "–û—Å—Ç–∞—Ç–æ–∫", "–ù–∞–ª–∏—á–∏–µ", "–ö–æ–ª-–≤–æ", "–°–∫–ª–∞–¥", "Qty".
6. `name_parts_col_indices` - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –ò–ù–î–ï–ö–°–û–í –∫–æ–ª–æ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞.
7. **–í–ê–ñ–ù–û:** –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–π—Å—ã –∏–º–µ—é—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –ï—Å–ª–∏ —Ç—ã –≤–∏–¥–∏—à—å, —á—Ç–æ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ó–∞–¥–≤–∏–∂–∫–∏ —á—É–≥—É–Ω–Ω—ã–µ") —è–≤–ª—è–µ—Ç—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö, —Ç—ã –¥–æ–ª–∂–µ–Ω —ç—Ç–æ —É—á–µ—Å—Ç—å.

**–ü—Ä–∏–º–µ—Ä –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–∂–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**
–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
```
,,,,
,–ó–∞–¥–≤–∏–∂–∫–∏ —á—É–≥—É–Ω–Ω—ã–µ,,,
,–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ,–î—É,–¶–µ–Ω–∞,–û—Å—Ç–∞—Ç–æ–∫
,30—á6–±—Ä,50,3500,10
,30—á6–±—Ä,80,4500,12
```
–û–∂–∏–¥–∞–µ–º—ã–π JSON:
{{
  "header_row_index": 2,
  "data_start_row_index": 3,
  "column_map": {{
    "price_col_index": 3,
    "stock_col_index": 4,
    "name_parts_col_indices": [1, 2]
  }}
}}
*–û–±—ä—è—Å–Ω–µ–Ω–∏–µ: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä—É–ø–ø—ã "–ó–∞–¥–≤–∏–∂–∫–∏ —á—É–≥—É–Ω–Ω—ã–µ" –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤—ã—à–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã, –ø–æ—ç—Ç–æ–º—É –¥–ª—è —Å–±–æ—Ä–∫–∏ –ø–æ–ª–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É –∏–µ—Ä–∞—Ä—Ö–∏–∏ –≤ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ.*

**–§—Ä–∞–≥–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:**
---
{sample_text}
---

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON-–æ–±—ä–µ–∫—Ç. –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤–Ω–µ JSON.
"""

    def _parse_llm_response(self, response_text: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç JSON –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ LLM."""
        try:
            match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if match:
                json_str = match.group(0)
                return json.loads(json_str)
            else:
                self.cascade_log.append("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ LLM.")
                return None
        except json.JSONDecodeError as e:
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM: {e}")
            return None

    def _get_auditor_verdict(self, sample_text: str, analysis_map: Dict) -> Optional[AuditorVerdict]:
        """–®–∞–≥ '–ö–æ–Ω—Å–∏–ª–∏—É–º'. –ê—É–¥–∏—Ç–æ—Ä (–≤—Ç–æ—Ä–æ–π LLM) –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—Ä—Ç—É –ê–Ω–∞–ª–∏—Ç–∏–∫–∞."""
        self.cascade_log.append("–®–∞–≥ 3: –ó–∞–ø—Ä–æ—Å –≤–µ—Ä–¥–∏–∫—Ç–∞ —É LLM-–ê—É–¥–∏—Ç–æ—Ä–∞.")
        
        prompt = f"""
–¢—ã ‚Äî —Å—Ç–∞—Ä—à–∏–π –∞—É–¥–∏—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É –º–ª–∞–¥—à–µ–≥–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞. –ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ JSON-–∫–∞—Ä—Ç–∞, –∫–æ—Ç–æ—Ä—É—é –æ–Ω —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª.

**–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç CSV):**
---
{sample_text}
---
**JSON-–∫–∞—Ä—Ç–∞ –æ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∞:**
---
{json.dumps(analysis_map, indent=2, ensure_ascii=False)}
---
**–¢–≤–æ—è –∑–∞–¥–∞—á–∞:**
–ü—Ä–æ–≤–µ—Ä—å JSON-–∫–∞—Ä—Ç—É –Ω–∞ –õ–û–ì–ò–ß–ï–°–ö–£–Æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å.
1. –£–±–µ–¥–∏—Å—å, —á—Ç–æ `price_col_index` –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–ª–æ–Ω–∫—É —Å —Ü–µ–Ω–∞–º–∏, –∞ –Ω–µ —Å —á–µ–º-—Ç–æ –¥—Ä—É–≥–∏–º (–∞—Ä—Ç–∏–∫—É–ª—ã, –ì–û–°–¢, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ). –î–æ–ø—É—Å–∫–∞–µ—Ç—Å—è, –µ—Å–ª–∏ –≤ –∫–æ–ª–æ–Ω–∫–µ –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –Ω–æ –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ü–µ–Ω—ã.
2. –£–±–µ–¥–∏—Å—å, —á—Ç–æ `name_parts_col_indices` –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è.

**–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON-–æ–±—ä–µ–∫—Ç —Å —Ç–≤–æ–∏–º –≤–µ—Ä–¥–∏–∫—Ç–æ–º:**
{{
  "is_correct": <true/false>,
  "reasoning": "<–∫—Ä–∞—Ç–∫–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ, –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ is_correct: false. –ï—Å–ª–∏ –≤—Å–µ –≤–µ—Ä–Ω–æ, –Ω–∞–ø–∏—à–∏ '–ö–∞—Ä—Ç–∞ –≤—ã–≥–ª—è–¥–∏—Ç –ª–æ–≥–∏—á–Ω–æ–π –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π.'>"
}}
"""
        try:
            response = self.llm.invoke(prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            verdict_json = self._parse_llm_response(response_text)
            if verdict_json:
                return AuditorVerdict(**verdict_json)
            return None
        except (ValidationError, Exception) as e:
            self.cascade_log.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤–µ—Ä–¥–∏–∫—Ç–∞ –∞—É–¥–∏—Ç–æ—Ä–∞: {e}")
            return None

    def _extract_products_with_map(self, df: pd.DataFrame, structure_map: PriceListMap) -> List[Dict]:
        """–≠—Ç–∞–ø "–•–∏—Ä—É—Ä–≥". –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–≤–∞—Ä—ã –∏–∑ DataFrame, –∏—Å–ø–æ–ª—å–∑—É—è –í–ê–õ–ò–î–ù–£–Æ –∫–∞—Ä—Ç—É —Å—Ç—Ä—É–∫—Ç—É—Ä—ã."""
        self.cascade_log.append("–®–∞–≥ 5: –ù–∞—á–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –≤–∞–ª–∏–¥–Ω–æ–π –∫–∞—Ä—Ç–µ.")
        
        header_row = structure_map.header_row_index
        data_start_row = structure_map.data_start_row_index
        
        col_map = structure_map.column_map
        name_parts_cols_indices = col_map.name_parts_col_indices
        price_col_index = col_map.price_col_index
        stock_col_index = col_map.stock_col_index
        
        # –î–∞–Ω–Ω—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å–æ —Å—Ç—Ä–æ–∫–∏ data_start_row
        data_df = df.iloc[data_start_row:].reset_index(drop=True)
        
        products = []
        current_group_name = ""
        current_subgroup_name = ""
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≥—Ä—É–ø–ø –≤ —Å—Ç—Ä–æ–∫–∞—Ö –≤—ã—à–µ data_start_row
        group_headers = self._find_group_headers(df, header_row, data_start_row)
        self.cascade_log.append(f"–ù–∞–π–¥–µ–Ω–æ {len(group_headers)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≥—Ä—É–ø–ø: {list(group_headers.values())}")

        for index, row in data_df.iterrows():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –ø–æ–¥–≥—Ä—É–ø–ø—ã
            if self._is_subgroup_header(row, name_parts_cols_indices, price_col_index):
                # –≠—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ–¥–≥—Ä—É–ø–ø—ã
                subgroup_parts = []
                for col_idx in name_parts_cols_indices:
                    if col_idx < len(row) and pd.notna(row.iloc[col_idx]):
                        part = str(row.iloc[col_idx]).strip()
                        if part and part.lower() not in ['nan', 'none', '']:
                            subgroup_parts.append(part)
                
                if subgroup_parts:
                    current_subgroup_name = " ".join(subgroup_parts)
                    self.cascade_log.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ–¥–≥—Ä—É–ø–ø—ã: '{current_subgroup_name}'")
                continue
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏–∑ —á–∞—Å—Ç–µ–π –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
            name_parts = []
            for col_idx in name_parts_cols_indices:
                if col_idx < len(row) and pd.notna(row.iloc[col_idx]):
                    part = str(row.iloc[col_idx]).strip()
                    if part and part.lower() not in ['nan', 'none', '']:
                        name_parts.append(part)
            
            if not name_parts:
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É –ø–æ –ø–æ–∑–∏—Ü–∏–∏ —Å—Ç—Ä–æ–∫–∏
            actual_row_index = data_start_row + index
            group_name = self._get_group_for_row(actual_row_index, group_headers)
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º —Å–ª–æ–∂–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –£–ê–ó
            full_name_parts = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            if group_name:
                full_name_parts.append(group_name)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–¥–≥—Ä—É–ø–ø—ã (–µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ)
            if current_subgroup_name and current_subgroup_name not in group_name:
                if not current_subgroup_name.isdigit():  # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–∞ –∫–∞–∫ –ø–æ–¥–≥—Ä—É–ø–ø—ã
                    full_name_parts.append(current_subgroup_name)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ —Å—Ç—Ä–æ–∫–∏
            model_name = name_parts[0] if name_parts else ""
            diameter = name_parts[1] if len(name_parts) > 1 else ""
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —É–∂–µ "–†—É")
            if model_name and not model_name.isdigit():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –†—É
                if '—Ä—É' not in model_name.lower():
                    full_name_parts.append(model_name)
                else:
                    # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –†—É, –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    full_name_parts.append(model_name)
            elif model_name and model_name.isdigit():
                # –ï—Å–ª–∏ model_name —ç—Ç–æ —á–∏—Å–ª–æ, —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –¥–∏–∞–º–µ—Ç—Ä
                diameter = model_name
                model_name = ""
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–º–µ—Ç—Ä —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º "–î—É"
            if diameter and diameter.isdigit():
                full_name_parts.append(f"–î—É {diameter}")
            elif diameter and not diameter.isdigit():
                # –ï—Å–ª–∏ –¥–∏–∞–º–µ—Ç—Ä –Ω–µ —á–∏—Å–ª–æ, –Ω–æ –µ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                full_name_parts.append(diameter)
            
            # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–≤–ª–µ–Ω–∏–∏ (–†—É) –≤ —Å—Ç—Ä–æ–∫–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            ru_info = self._find_ru_info(df, actual_row_index, col_map.price_col_index)
            if ru_info and not any('—Ä—É' in part.lower() for part in full_name_parts):
                full_name_parts.append(ru_info)
            
            full_name = " ".join(full_name_parts).strip()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É –ø–æ –∏–Ω–¥–µ–∫—Å—É
            price = None
            if price_col_index is not None and price_col_index < len(row):
                price = self._clean_price(row.iloc[price_col_index])

            if not full_name or len(full_name) < 3:
                continue

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É
            stock = '–≤ –Ω–∞–ª–∏—á–∏–∏'
            if stock_col_index is not None and stock_col_index < len(row):
                stock = self._clean_stock(row.iloc[stock_col_index])
            else:
                # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ –æ—Å—Ç–∞—Ç–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—â–µ–º –µ–µ "–ø–æ —Å–º—ã—Å–ª—É" –≤ —Å—Ç—Ä–æ–∫–µ
                stock_val_from_row = next((str(v) for v in row if isinstance(v, str) and any(w in v.lower() for w in ['–Ω–∞–ª–∏—á–∏', '–∑–∞–∫–∞–∑'])), '–≤ –Ω–∞–ª–∏—á–∏–∏')
                stock = self._clean_stock(stock_val_from_row)

            products.append({"name": full_name, "price": price, "stock": stock})
            
        self.cascade_log.append(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏")
        return products

    def _find_group_headers(self, df: pd.DataFrame, header_row: int, data_start_row: int) -> Dict[int, str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≥—Ä—É–ø–ø –≤–æ –≤—Å–µ–º —Ñ–∞–π–ª–µ."""
        group_headers = {}
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≥—Ä—É–ø–ø –≤–æ –≤—Å–µ–º —Ñ–∞–π–ª–µ
        for row_idx in range(len(df)):
            if row_idx == header_row:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫
                continue
                
            row = df.iloc[row_idx]
            
            # –ò—â–µ–º —è—á–µ–π–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –≥—Ä—É–ø–ø
            for col_idx, value in enumerate(row):
                if pd.notna(value) and isinstance(value, str):
                    value = str(value).strip()
                    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≥—Ä—É–ø–ø
                    if (len(value) > 8 and  # –£–≤–µ–ª–∏—á–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                        any(keyword in value.lower() for keyword in [
                            '–∑–∞–¥–≤–∏–∂–∫', '—Ñ–ª–∞–Ω–µ—Ü', '–æ—Ç–≤–æ–¥', '—Ç—Ä–æ–π–Ω–∏–∫', '–ø–µ—Ä–µ—Ö–æ–¥', '–∫–ª–∞–ø–∞–Ω', 
                            '–∫—Ä–∞–Ω', '–∑–∞—Ç–≤–æ—Ä', '–≤–µ–Ω—Ç–∏–ª', '—Ñ–∏–ª—å—Ç—Ä', '–º—É—Ñ—Ç–∞', '—á—É–≥—É–Ω', '—Å—Ç–∞–ª—å',
                            '–∞—Ä–º–∞—Ç—É—Ä–∞', '—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥', '—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ', '–∫—Ä–µ–ø–µ–∂', '–±–æ–ª—Ç', '–≥–∞–π–∫–∞',
                            '—à–∞–π–±–∞', '–ø—Ä–æ–∫–ª–∞–¥–∫–∞', '—É–ø–ª–æ—Ç–Ω–µ–Ω–∏–µ', '—Ä–µ–¥—É–∫—Ç–æ—Ä', '–Ω–∞—Å–æ—Å', '–∫–æ–º–ø–µ–Ω—Å–∞—Ç–æ—Ä',
                            '–æ–ø–æ—Ä–∞', '–ø–æ–¥–≤–µ—Å–∫–∞', '–∏–∑–æ–ª—è—Ü–∏—è', '—Ç–µ–ø–ª–æ–∏–∑–æ–ª—è—Ü–∏—è', '—Ü–µ–ø—å', '–∫–∞–Ω–∞—Ç',
                            '—Å—Ç—Ä–æ–ø', '—Ç–∞–∫–µ–ª–∞–∂', '–≥—Ä—É–∑–æ–ø–æ–¥—ä–µ–º'
                        ]) and
                        value.lower() not in ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '—Ü–µ–Ω–∞', '–æ—Å—Ç–∞—Ç–æ–∫', '–∞—Ä—Ç–∏–∫—É–ª', '–≥–æ—Å—Ç', '—Ç—É']):
                        group_headers[row_idx] = value
                        self.cascade_log.append(f"–ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä—É–ø–ø—ã –≤ —Å—Ç—Ä–æ–∫–µ {row_idx}: '{value}'")
                        break
        
        return group_headers

    def _get_group_for_row(self, row_index: int, group_headers: Dict[int, str]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫ –∫–∞–∫–æ–π –≥—Ä—É–ø–ø–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Å—Ç—Ä–æ–∫–∞ —Å —Ç–æ–≤–∞—Ä–æ–º."""
        # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä—É–ø–ø—ã –≤—ã—à–µ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–∏
        applicable_headers = [(header_row, name) for header_row, name in group_headers.items() if header_row < row_index]
        
        if applicable_headers:
            # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –±–ª–∏–∑–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
            _, group_name = max(applicable_headers, key=lambda x: x[0])
            return group_name
        
        return ""

    def _is_subgroup_header(self, row: pd.Series, name_cols: List[int], price_col: Optional[int]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –ø–æ–¥–≥—Ä—É–ø–ø—ã."""
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ–¥–≥—Ä—É–ø–ø—ã –æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ, –Ω–æ –Ω–µ –∏–º–µ–µ—Ç —Ü–µ–Ω—ã
        has_name = False
        has_price = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        for col_idx in name_cols:
            if col_idx < len(row) and pd.notna(row.iloc[col_idx]):
                value = str(row.iloc[col_idx]).strip()
                if value and value.lower() not in ['nan', 'none', '']:
                    has_name = True
                    break
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ü–µ–Ω—ã
        if price_col is not None and price_col < len(row):
            price_val = row.iloc[price_col]
            if pd.notna(price_val):
                price_str = str(price_val).strip()
                # –ï—Å–ª–∏ –≤ –∫–æ–ª–æ–Ω–∫–µ —Ü–µ–Ω—ã –µ—Å—Ç—å —á–∏—Å–ª–æ, —ç—Ç–æ –Ω–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                if re.search(r'\d+', price_str):
                    has_price = True
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ–¥–≥—Ä—É–ø–ø—ã: –µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ, –Ω–æ –Ω–µ—Ç —Ü–µ–Ω—ã
        return has_name and not has_price

    def _find_ru_info(self, df: pd.DataFrame, row_index: int, price_col_index: Optional[int]) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–≤–ª–µ–Ω–∏–∏ (–†—É) –¥–ª—è —Ç–æ–≤–∞—Ä–∞ –ø–æ –∫–æ–ª–æ–Ω–∫–µ —Ü–µ–Ω—ã."""
        if price_col_index is None:
            return ""
        
        # –ò—â–µ–º –≤ —Å—Ç—Ä–æ–∫–∞—Ö –≤—ã—à–µ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –†—É
        for check_row in range(max(0, row_index - 5), row_index):
            if check_row < len(df):
                row = df.iloc[check_row]
                if price_col_index < len(row) and pd.notna(row.iloc[price_col_index]):
                    value = str(row.iloc[price_col_index]).strip()
                    if '—Ä—É' in value.lower() and any(char.isdigit() for char in value):
                        return value
        
        return ""

    def _clean_price(self, price_val: Any) -> Optional[float]:
        if pd.isna(price_val): return None
        try:
            price_str = re.sub(r'[^\d,.]', '', str(price_val)).replace(',', '.')
            return float(price_str) if price_str else None
        except (ValueError, TypeError): return None

    def _clean_stock(self, stock_val: Any) -> str:
        if pd.isna(stock_val): return "–Ω–µ —É–∫–∞–∑–∞–Ω"
        stock_str = str(stock_val).lower().strip()
        if any(w in stock_str for w in ['–Ω–∞–ª–∏—á–∏', '–µ—Å—Ç—å', '+']): return "–≤ –Ω–∞–ª–∏—á–∏–∏"
        if any(w in stock_str for w in ['–∑–∞–∫–∞–∑', '–æ–∂–∏–¥']): return "–ø–æ–¥ –∑–∞–∫–∞–∑"
        numbers = re.findall(r'\d+', stock_str)
        return numbers[0] if numbers else "–Ω–µ —É–∫–∞–∑–∞–Ω"

    def get_cascade_summary(self, result: Dict) -> str:
        summary_lines = ["--- –°–≤–æ–¥–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ ---"]
        summary_lines.append(f"–°—Ç–∞—Ç—É—Å: {'–£—Å–ø–µ—à–Ω–æ' if result.get('success') else '–û—à–∏–±–∫–∞'}")
        summary_lines.append(f"–§–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥: {result.get('final_method', 'N/A')}")
        summary_lines.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(result.get('products', []))}")
        if not result.get('success'):
            summary_lines.append(f"–°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ: {result.get('error', 'N/A')}")
        summary_lines.append("\n--- –î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ ---")
        summary_lines.extend(result.get('cascade_log', ["–õ–æ–≥ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç."]))
        return "\n".join(summary_lines)

    def _process_with_heuristics(self, file_path: str) -> Dict:
        log = []
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–∏—Å—Ç–æ–≤ –≤ Excel
            file_ext = os.path.splitext(file_path)[1].lower()
            all_products = []
            
            if file_ext in ['.xlsx', '.xls']:
                excel_file = pd.ExcelFile(file_path)
                sheet_names = excel_file.sheet_names
                log.append(f"–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {len(sheet_names)} –ª–∏—Å—Ç(–æ–≤): {sheet_names}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ª–∏—Å—Ç
                for sheet_name in sheet_names:
                    log.append(f"\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–∏—Å—Ç–∞ '{sheet_name}' ---")
                    try:
                        df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)
                        log.append(f"–õ–∏—Å—Ç '{sheet_name}' –ø—Ä–æ—á–∏—Ç–∞–Ω: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
                        
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ª–∏—Å—Ç —Ç–µ–º–∏ –∂–µ –º–µ—Ç–æ–¥–∞–º–∏
                        products = self._process_single_sheet(df, sheet_name, log)
                        all_products.extend(products)
                        
                    except Exception as sheet_error:
                        log.append(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏—Å—Ç–∞ '{sheet_name}': {sheet_error}")
                        continue
                
                if all_products:
                    log.append(f"\n–í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ —Å–æ –≤—Å–µ—Ö –ª–∏—Å—Ç–æ–≤: {len(all_products)}")
                    return {"success": True, "products": all_products, "log": log}
                else:
                    return {"success": False, "products": [], "log": log, "error": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∏ –Ω–∞ –æ–¥–Ω–æ–º –ª–∏—Å—Ç–µ"}
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —á–∏—Ç–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω–æ
                df = pd.read_excel(file_path, header=None)
                log.append(f"–§–∞–π–ª {file_path} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω –≤ DataFrame.")
                products = self._process_single_sheet(df, "main", log)
                return {"success": True, "products": products, "log": log} if products else {"success": False, "products": [], "log": log, "error": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤"}
                
        except Exception as e:
            log.append(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")
            return {"success": False, "products": [], "log": log, "error": f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}"}
    
    def _process_single_sheet(self, df: pd.DataFrame, sheet_name: str, log: List[str]) -> List[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –ª–∏—Å—Ç–∞ Excel"""
        header_row_index, header = self._find_header_row(df, log)
        if header_row_index is None:
            log.append(f"–õ–∏—Å—Ç '{sheet_name}': –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç—Ä–æ–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–∞.")
            return []  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –∞ –Ω–µ —Å–ª–æ–≤–∞—Ä—å
        
        df.columns = [f"col_{i}" for i in range(len(df.columns))]
        header_map = {f"col_{i}": str(h) for i, h in enumerate(header)}
        
        data_df = df.iloc[header_row_index + 1:].reset_index(drop=True)
        log.append(f"–õ–∏—Å—Ç '{sheet_name}': –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã, –Ω–∞—á–∏–Ω–∞—è —Å–æ —Å—Ç—Ä–æ–∫–∏ {header_row_index + 1}.")

        name_col, price_col, stock_col = self._map_columns(header_map, data_df, log)

        if not name_col or not price_col:
            log.append(f"–õ–∏—Å—Ç '{sheet_name}': –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' –∏–ª–∏ '–¶–µ–Ω–∞'.")
            return []  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫

        products = self._extract_products_with_subheaders(data_df, name_col, price_col, stock_col, log, header_map)

        if not products:
            log.append(f"–õ–∏—Å—Ç '{sheet_name}': –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞.")
            return []
        
        log.append(f"–õ–∏—Å—Ç '{sheet_name}': –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤.")
        return products

    def _find_header_row(self, df: pd.DataFrame, log: List[str]) -> Tuple[Optional[int], Optional[List[str]]]:
        header_keywords = ['–Ω–∞–∏–º–µ–Ω', '—Ç–æ–≤–∞—Ä', '—Ü–µ–Ω–∞', '–∫–æ–ª-–≤–æ', '–æ—Å—Ç–∞—Ç', '–∞—Ä—Ç–∏–∫—É–ª', '—Ä—É–±', '–≥–æ—Å—Ç', '–Ω-—Ä–∞', '–æ–ø–∏—Å–∞–Ω–∏–µ']
        best_row_index = -1
        max_matches = 0

        for i, row in df.head(20).iterrows():
            row_str = ' '.join(str(x) for x in row.dropna() if x).lower()
            if not row_str: continue
            
            matches = sum(1 for kw in header_keywords if kw in row_str)
            
            if matches > 1 and matches > max_matches:
                max_matches = matches
                best_row_index = i
        
        if best_row_index != -1:
            log.append(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–∏–Ω–¥–µ–∫—Å {best_row_index}) —Å {max_matches} —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏.")
            return best_row_index, list(df.iloc[best_row_index])
        
        log.append("–°—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞.")
        return 0, list(df.iloc[0])

    def _map_columns(self, header_map: Dict, df: pd.DataFrame, log: List[str]) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        price_kw = ['—Ü–µ–Ω–∞', 'price', '—Å—Ç–æ–∏–º', 'cost', 'value', '—Ä—É–±', 'rub', '—Å—É–º–º–∞']
        stock_kw = ['–æ—Å—Ç–∞—Ç', '–∫–æ–ª-–≤–æ', '–Ω–∞–ª–∏—á', 'stock', 'qty', 'amount', 'balance', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '—Å–∫–ª–∞–¥']
        name_kw = ['–Ω–∞–∏–º–µ–Ω', '—Ç–æ–≤–∞—Ä', 'product', 'item', '–æ–ø–∏—Å–∞–Ω', 'nomenkl', '–Ω–∞–∑–≤', '–ø—Ä–æ–¥—É–∫—Ç']

        def find_header(keywords):
            for col_idx, header_name in header_map.items():
                header_str = str(header_name).lower()
                if header_str != 'nan' and any(k in header_str for k in keywords):
                    return col_idx
            return None

        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        name_col = find_header(name_kw)
        price_col = find_header(price_kw)
        stock_col = find_header(stock_kw)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–ª–æ–Ω–∫—É —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º, –∏—â–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        if not name_col:
            log.append("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º. –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É...")
            for col_idx in header_map.keys():
                text_like_count = 0
                for i in range(min(10, len(df))):
                    val = str(df.iloc[i].get(col_idx, '')).strip()
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ —Ç–µ–∫—Å—Ç (–Ω–µ —á–∏—Å–ª–æ –∏ –Ω–µ –ø—É—Å—Ç–æ–µ)
                    if val and val != 'nan' and re.search(r'[–∞-—è–ê-–Øa-zA-Z]', val) and not re.match(r'^\d+[\.,\d]*$', val):
                        text_like_count += 1
                
                if text_like_count >= 5:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ –ø–æ–ª–æ–≤–∏–Ω—ã –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Ö–æ–∂–∏ –Ω–∞ —Ç–µ–∫—Å—Ç
                    name_col = col_idx
                    log.append(f"–í–µ—Ä–æ—è—Ç–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –Ω–∞–π–¥–µ–Ω–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É: {col_idx}")
                    break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–ª–æ–Ω–∫—É —Å —Ü–µ–Ω–æ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –¥–∞–Ω–Ω—ã–º
        if not price_col:
            log.append("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Ü–µ–Ω—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º. –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É...")
            for col_idx in header_map.keys():
                if col_idx == name_col:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º
                    continue
                    
                price_like_count = 0
                for i in range(min(10, len(df))):
                    val = str(df.iloc[i].get(col_idx, '')).strip()
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ —Ü–µ–Ω—É (—Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã, —Ç–æ—á–∫–∏, –∑–∞–ø—è—Ç—ã–µ)
                    if val and val != 'nan' and re.match(r'^\d+[\.,\d\s]*$', val):
                        price_like_count += 1
                
                if price_like_count >= 5:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ –ø–æ–ª–æ–≤–∏–Ω—ã –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Ö–æ–∂–∏ –Ω–∞ —Ü–µ–Ω—ã
                    price_col = col_idx
                    log.append(f"–í–µ—Ä–æ—è—Ç–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ —Ü–µ–Ω—ã –Ω–∞–π–¥–µ–Ω–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É: {col_idx}")
                    break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–ª–æ–Ω–∫—É —Å –æ—Å—Ç–∞—Ç–∫–æ–º, –∏—â–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        if not stock_col:
            log.append("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º. –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É...")
            for col_idx in header_map.keys():
                if col_idx in [name_col, price_col]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                    continue
                    
                stock_like_count = 0
                for i in range(min(10, len(df))):
                    val = str(df.iloc[i].get(col_idx, '')).strip().lower()
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ –æ—Å—Ç–∞—Ç–æ–∫
                    if val and val != 'nan' and (re.match(r'^\d+$', val) or any(w in val for w in ['–Ω–∞–ª–∏—á–∏', '–∑–∞–∫–∞–∑', '–µ—Å—Ç—å', '–Ω–µ—Ç'])):
                        stock_like_count += 1
                
                if stock_like_count >= 3:  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –∫—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –æ—Å—Ç–∞—Ç–∫–æ–≤
                    stock_col = col_idx
                    log.append(f"–í–µ—Ä–æ—è—Ç–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É: {col_idx}")
                    break
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if not name_col:
            name_col = 'col_0'
            log.append("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É –∫–∞–∫ –∫–æ–ª–æ–Ω–∫—É –Ω–∞–∑–≤–∞–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        log.append(f"–§–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥: Name='{header_map.get(name_col)}' ({name_col}), Price='{header_map.get(price_col)}' ({price_col}), Stock='{header_map.get(stock_col)}' ({stock_col})")
        
        return name_col, price_col, stock_col

    def _extract_products_with_subheaders(self, df: pd.DataFrame, name_col: str, price_col: str, stock_col: Optional[str], log: List[str], header_map: Dict) -> List[Dict]:
        products = []
        current_subheader = ""
        for index, row in df.iterrows():
            name = str(row.get(name_col, "")).strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å "nan" –∏–ª–∏ –ø—É—Å—Ç—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
            if not name or name.lower() in ['nan', 'none', '']:
                continue
                
            is_subheader = name and all(pd.isna(v) or str(v).strip() == "" or str(v).strip().lower() == 'nan' for k, v in row.items() if k != name_col)
            
            if is_subheader:
                current_subheader = name
                log.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫: '{current_subheader}'")
                continue

            full_name = f"{current_subheader} {name}".strip()
            
            price = 0
            price_raw = str(row.get(price_col, "")).strip()
            if price_raw and price_raw not in ['nan', 'None', '-', '']:
                # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                logger.debug(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–Ω—É: '{price_raw}'")
                
                # –£–¥–∞–ª—è–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤–∫–ª—é—á–∞—è –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
                price_raw = price_raw.replace('\xa0', ' ').replace('\u00a0', ' ')
                
                # –ï—Å–ª–∏ —Ü–µ–Ω–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥—Ä–æ–±—å —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä "1 234,56")
                # –∏–ª–∏ —á–µ—Ä–µ–∑ —Ç–æ—á–∫—É –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Ç—ã—Å—è—á (–Ω–∞–ø—Ä–∏–º–µ—Ä "1.234,56")
                price_raw = price_raw.replace(' ', '')
                
                # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É –¥–ª—è –¥–µ—Å—è—Ç–∏—á–Ω—ã—Ö
                if ',' in price_raw and '.' in price_raw:
                    # –ï—Å–ª–∏ –µ—Å—Ç—å –∏ —Ç–æ—á–∫–∞ –∏ –∑–∞–ø—è—Ç–∞—è, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ —Ç–æ—á–∫–∞ - —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Ç—ã—Å—è—á
                    price_raw = price_raw.replace('.', '').replace(',', '.')
                else:
                    price_raw = price_raw.replace(',', '.')
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞ –∏ —Ç–æ—á–∫—É
                clean_price = re.sub(r'[^\d\.]', '', price_raw)
                
                if clean_price:
                    try:
                        price = float(clean_price)
                        logger.debug(f"–£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–∞ —Ü–µ–Ω–∞: {price}")
                    except (ValueError, TypeError) as e:
                        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ü–µ–Ω—É '{clean_price}': {e}")
            
            stock = 100
            if stock_col and pd.notna(row.get(stock_col)):
                stock_raw = str(row[stock_col]).lower().strip()
                if stock_raw != 'nan':
                    if any(w in stock_raw for w in ['–Ω–µ—Ç', '0', '–ø–æ–¥ –∑–∞–∫–∞–∑', '–æ–∂–∏–¥', '–æ—Ç—Å—É—Ç']): 
                        stock = 0
                    elif any(w in stock_raw for w in ['–µ—Å—Ç—å', '–≤ –Ω–∞–ª–∏—á–∏–∏', '–Ω–∞–ª–∏—á', '–º–Ω–æ–≥–æ']): 
                        stock = 100
                    else:
                        stock_numbers = re.findall(r'\d+', stock_raw)
                        if stock_numbers: 
                            stock = int(stock_numbers[0])

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–≤–∞—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –≤–∞–ª–∏–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
            if full_name and full_name.strip() and full_name.lower() != 'nan':
                products.append({"name": full_name, "price": price, "stock": stock})
                if price == 0:
                    logger.warning(f"–¢–æ–≤–∞—Ä —Å –Ω—É–ª–µ–≤–æ–π —Ü–µ–Ω–æ–π: {full_name[:50]}...")
        
        log.append(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤.")
        return products
    
    def _read_file_safely(self, file_path: str) -> Optional[pd.DataFrame]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        try:
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext == '.csv':
                # –î–ª—è CSV –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                for encoding in ['utf-8', 'cp1251', 'latin1']:
                    try:
                        return pd.read_csv(file_path, encoding=encoding, dtype=str)
                    except:
                        continue
                        
            elif file_ext in ['.xlsx', '.xls']:
                # –î–ª—è Excel –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –¥–≤–∏–∂–∫–∏
                if file_ext == '.xls':
                    try:
                        return pd.read_excel(file_path, header=None, dtype=str, engine='xlrd')
                    except:
                        try:
                            return pd.read_excel(file_path, header=None, dtype=str, engine='openpyxl')
                        except:
                            return pd.read_excel(file_path, header=None, dtype=str)
                else:
                    return pd.read_excel(file_path, header=None, dtype=str)
                    
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return None
    
    def _select_best_overall_result(self, level1_result: Dict, level2_result: Dict) -> Optional[Dict]:
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π"""
        candidates = []
        
        if level1_result.get('success') and level1_result.get('products'):
            candidates.append({
                'method': 'level1_smart',
                'products': level1_result['products'],
                'count': len(level1_result['products'])
            })
            
        if level2_result.get('success') and level2_result.get('products'):
            candidates.append({
                'method': 'level2_bruteforce', 
                'products': level2_result['products'],
                'count': len(level2_result['products'])
            })
        
        if not candidates:
            return None
            
        # –í—ã–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–æ–≤–∞—Ä–æ–≤
        best = max(candidates, key=lambda x: x['count'])
        return best
        
    # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ level1_processor –∏ level2_processor
    # def process_file_cascade(self, file_path: str, file_name: str = "") -> Dict:
    #     """
    #     –ö–∞—Å–∫–∞–¥–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å —Ç—Ä–µ–º—è —É—Ä–æ–≤–Ω—è–º–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    #     """
    #     logger.info(f"üéØ –ö–ê–°–ö–ê–î–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: {file_name}")
    #     
    #     result = {
    #         'success': False,
    #         'final_method': None,
    #         'products': [],
    #         'cascade_log': [],
    #         'all_attempts': {}
    #     }
    #     
    #     try:
    #         # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
    #         df = self._read_file_safely(file_path)
    #         if df is None:
    #             result['cascade_log'].append("‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞")
    #             return result
    #         
    #         # –£–†–û–í–ï–ù–¨ 1: –£–ú–ù–ê–Ø –°–ò–°–¢–ï–ú–ê
    #         logger.info("üî• –£–†–û–í–ï–ù–¨ 1: –£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞")
    #         level1_result = self.level1_processor.process_price_list(df, file_name)
    #         result['all_attempts']['level1'] = level1_result
    #         
    #         if level1_result['success'] and len(level1_result['products']) >= 5:
    #             # –£—Å–ø–µ—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ 1!
    #             result['success'] = True
    #             result['final_method'] = 'level1_smart'
    #             result['products'] = level1_result['products']
    #             result['cascade_log'].append(f"‚úÖ –£–†–û–í–ï–ù–¨ 1 –£–°–ü–ï–•: {len(level1_result['products'])} —Ç–æ–≤–∞—Ä–æ–≤")
    #             logger.info(f"‚úÖ –£—Ä–æ–≤–µ–Ω—å 1 —É—Å–ø–µ—à–µ–Ω: {len(level1_result['products'])} —Ç–æ–≤–∞—Ä–æ–≤")
    #             return result
    #         else:
    #             result['cascade_log'].append(f"‚ö†Ô∏è –£–†–û–í–ï–ù–¨ 1: {len(level1_result.get('products', []))} —Ç–æ–≤–∞—Ä–æ–≤ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)")
    #             
    #         # –£–†–û–í–ï–ù–¨ 2: –ë–†–£–¢–§–û–†–° –ê–ù–ê–õ–ò–ó
    #         logger.info("üî• –£–†–û–í–ï–ù–¨ 2: –ë—Ä—É—Ç—Ñ–æ—Ä—Å –∞–Ω–∞–ª–∏–∑")
    #         level2_result = self.level2_processor.process_complex_file(file_path, file_name)
    #         result['all_attempts']['level2'] = level2_result
    #         
    #         if level2_result['success'] and len(level2_result['products']) > 0:
    #             # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Ä–æ–≤–Ω—è 1 –∏ 2
    #             level1_count = len(level1_result.get('products', []))
    #             level2_count = len(level2_result['products'])
    #             
    #             if level2_count > level1_count:
    #                 # –£—Ä–æ–≤–µ–Ω—å 2 –ª—É—á—à–µ
    #                 result['success'] = True
    #                 result['final_method'] = 'level2_bruteforce'
    #                 result['products'] = level2_result['products']
    #                 result['cascade_log'].append(f"‚úÖ –£–†–û–í–ï–ù–¨ 2 –õ–£–ß–®–ï: {level2_count} —Ç–æ–≤–∞—Ä–æ–≤ (vs {level1_count})")
    #                 logger.info(f"‚úÖ –£—Ä–æ–≤–µ–Ω—å 2 –ª—É—á—à–µ: {level2_count} vs {level1_count} —Ç–æ–≤–∞—Ä–æ–≤")
    #                 return result
    #             elif level1_count > 0:
    #                 # –£—Ä–æ–≤–µ–Ω—å 1 –≤—Å–µ –∂–µ –ª—É—á—à–µ
    #                 result['success'] = True  
    #                 result['final_method'] = 'level1_smart'
    #                 result['products'] = level1_result['products']
    #                 result['cascade_log'].append(f"‚úÖ –£–†–û–í–ï–ù–¨ 1 –õ–£–ß–®–ï: {level1_count} —Ç–æ–≤–∞—Ä–æ–≤ (vs {level2_count})")
    #                 return result
    #         else:
    #             result['cascade_log'].append(f"‚ö†Ô∏è –£–†–û–í–ï–ù–¨ 2: {len(level2_result.get('products', []))} —Ç–æ–≤–∞—Ä–æ–≤")
    #             
    #         # –£–†–û–í–ï–ù–¨ 3: –ü–û–õ–ù–´–ô LLM –ê–ù–ê–õ–ò–ó (–ø–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞)
    #         logger.info("üî• –£–†–û–í–ï–ù–¨ 3: –ü–æ–ª–Ω—ã–π LLM –∞–Ω–∞–ª–∏–∑")
    #         result['cascade_log'].append("‚è≥ –£–†–û–í–ï–ù–¨ 3: –¢—Ä–µ–±—É–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏")
    #         
    #         # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ - –±–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ —É—Ä–æ–≤–Ω–µ–π 1-2
    #         best_result = self._select_best_overall_result(level1_result, level2_result)
    #         if best_result:
    #             result['success'] = True
    #             result['final_method'] = best_result['method']
    #             result['products'] = best_result['products']
    #             result['cascade_log'].append(f"‚úÖ –õ–£–ß–®–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢: {best_result['method']} - {len(best_result['products'])} —Ç–æ–≤–∞—Ä–æ–≤")
    #         else:
    #             result['cascade_log'].append("‚ùå –í–°–ï –£–†–û–í–ù–ò –ù–ï–£–°–ü–ï–®–ù–´")
    #             
    #     except Exception as e:
    #         logger.error(f"–û—à–∏–±–∫–∞ –∫–∞—Å–∫–∞–¥–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    #         result['cascade_log'].append(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {str(e)}")
    #         
    #     return result 